{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroimaging with Dipy\n",
    "\n",
    "[Dipy Home Page](http://nipy.org/dipy/)\n",
    "\n",
    "In this tutorial we'll go through some basic image processing with Dipy. We'll go through how to fit different diffusion models (DTI & CSD) and how to compute tractography in different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image Data\n",
    "Lets use a subject from the ADNI3 dataset. We'll be reconstrucing tensors and computing tacks, so we'll need need the preprocessed diffusion image, bvecs, and bvals.  Nibabel is a python package that helps with nifti IO as we just saw. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table\n",
    "\n",
    "# File paths : on grid this is /ifshome/ccorbin/notebooks/data/blah\n",
    "f_dwi ='./data/002_S_0413_1_DTI_EC_EPI.nii.gz'\n",
    "f_bvals = './data/002_S_0413_1_bval.txt'\n",
    "f_bvecs = './data/002_S_0413_1_rotated_bvec_EC.txt'\n",
    "\n",
    "# Load DWI\n",
    "img = nib.load(f_dwi)\n",
    "data = img.get_data()\n",
    "print(img.affine)\n",
    "print(data.shape)\n",
    "\n",
    "# Load Bvals and Bvecs\n",
    "bvals, bvecs = read_bvals_bvecs(f_bvals, f_bvecs)\n",
    "print(bvals)\n",
    "print(bvecs)\n",
    "gtab = gradient_table(bvals, bvecs, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bvecs Needs to be Unit Vectors\n",
    "It would be helpful if Dipy actually converted to unit vectors instead of just spitting out an error when they aren't, but we can do that ourselves with just a few lines of code. Maybe we'll make a pull request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bvecs = np.loadtxt(f_bvecs)\n",
    "bvecs = [np.divide(b, np.linalg.norm(b)) if np.any(b != 0) else b for b in bvecs]\n",
    "print(np.asarray(bvecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Tensors to the Data\n",
    "Now that we've normalized our bvecs we can create a gradient table with the default tolerance level.  Lets use the gradient table and diffusion data to fit tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.reconst.dti import TensorModel\n",
    "gtab = gradient_table(bvals, bvecs, atol=1e-2)\n",
    "print(gtab.bvecs)\n",
    "\n",
    "# Tensor Model\n",
    "model = TensorModel(gtab)\n",
    "ten = model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at some TensorFit attributes\n",
    "We can access eigenvectors for each voxel, FA values, and the diffusion tensor itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Eigenvectors\")\n",
    "print(ten.evecs.shape)\n",
    "print(ten.evecs[50][50][50])\n",
    "print(\"Fractional Anisotropy\")\n",
    "shape = ten.fa.shape\n",
    "print(ten.fa[shape[0] //2 ][shape[1] // 2 ][shape[2] // 2])\n",
    "print(\"Diffusion Tensor\")\n",
    "print(ten.quadratic_form[50, 40, 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Tensors\n",
    "Lets visualize the tensors we just reconstructed on a small slice of the brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.reconst.dti import color_fa\n",
    "from dipy.data import get_sphere\n",
    "from dipy.viz import window, actor\n",
    "\n",
    "fa = ten.fa\n",
    "RGB = color_fa(fa, ten.evecs)\n",
    "sphere = get_sphere('symmetric724')\n",
    "ren = window.Renderer()\n",
    "evals = ten.evals[:, :, 55:56]\n",
    "evecs = ten.evecs[:, :, 55:56]\n",
    "cfa = RGB[:, :, 55:56]\n",
    "cfa /= cfa.max()\n",
    "ren.add(actor.tensor_slicer(evals, evecs, scalar_colors=cfa, sphere=sphere, scale=0.3))\n",
    "window.show(ren)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Compute Some Tracks With Dipy's EuDX \n",
    "EuDX is Dipy's simplest way of computing tracks. Deterministic only, but can propagate with information from a variety of reconstruction techniques.  Here we'll just use tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.tracking.eudx import EuDX\n",
    "from dipy.reconst.dti import quantize_evecs\n",
    "from dipy.data import get_sphere\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "from dipy.io.streamline import save_trk\n",
    "\n",
    "sphere = get_sphere('symmetric724')\n",
    "ind = quantize_evecs(ten.evecs, sphere.vertices)\n",
    "eu = EuDX(a=ten.fa, ind=ind, seeds=1000000, odf_vertices=sphere.vertices, a_low=.2)\n",
    "streamlines = Streamlines(eu)\n",
    "save_trk(\"eudx.trk\", streamlines, img.affine, vox_size=img.header.get_zooms()[0:3], shape=data.shape[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the number of streamlines generated\n",
    "print(len(streamlines))\n",
    "# Print the first streamline\n",
    "print(streamlines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize tracks\n",
    "We can visualize these streamlines with Dipy's Viz API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "from dipy.viz.colormap import line_colors\n",
    "\n",
    "color = line_colors(streamlines)\n",
    "streamlines_actor = actor.line(streamlines, line_colors(streamlines))\n",
    "r = window.Renderer()\n",
    "r.add(streamlines_actor)\n",
    "window.show(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Look at how streamlines are represented in python\n",
    "A streamline is represented as a Nx3 numpy array.  A collection of these (typically list) is how we interact with \"streamlines\" - Dipy now has integrated with [Nibabel's new streamlines api](http://nipy.org/nibabel/reference/nibabel.streamlines.html), so there's actually a streamlines class in Dipy which inherits from Nibabel's ArraySequence class. \n",
    "\n",
    "## Streamline Spaces\n",
    "There are three main different spaces / coordinate systems that you'll find streamlines in.  Understanding these three spaces and how to convert from one space to another is pretty essential when you want to manipulate streamlines (clustering streamines / analyzing microstructural measures along them / connectivity stuff)\n",
    "\n",
    "### Voxel Space (aka Grid Space)\n",
    "Streamines in this space go hand in hand with the voxel coordinate system of the image the streamlines were generated from.  A streamline with point (0, 0, 0) would reside at the center fo the first voxel in the image.  In Dipy if you compute tracks with Eudx (or local_tracking which is next) and don't feed in an affine, then your tracks will be returned in this space. \n",
    "\n",
    "### Voxmm space (Trackvis space)\n",
    "This is the space that Trackvis (the visualization program) stores streamlines.  Its defined by the the grid coordinates * voxel size in mm + voxel_size / 2. The plus one is a shift b/c trackvis interprets streamline voxel coordinates such that the origin refers to the top corner of the first voxel To transform streamlines that are in voxel space to Voxmm space you simply multiply each streamline point by its voxel size (with a translation of half the voxel size). \n",
    "\n",
    "### RAS+ and mm space (aka World Coordinates)\n",
    "When you look at the qfrom of a nifti image - its the transformation that takes the image from their voxel coordinates to World coordinates (which is either scanner coordinates or coordinate in relation to some atlas).  Streamlines can also be stored and represented in this space by applying the qform from a nifti (affine) to each streamline point. The origin is going to be near the middle of the brain. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Streamlines in Voxel Space (grid space)\n",
    "streamlines_voxel = streamlines\n",
    "print(streamlines_voxel[0])\n",
    "\n",
    "# Loading Streamlines in Voxmm Space with Nibabel's depricated trackvis library\n",
    "tracks, hdr = nib.trackvis.read('eudx.trk')\n",
    "streamlines_voxmm = [trk[0] for trk in tracks]\n",
    "print(streamlines_voxmm[0])\n",
    "\n",
    "# Loading streamlines with Nibabel's new streamlines api\n",
    "tractogram = nib.streamlines.load('eudx.trk')\n",
    "streamlines_rasmm = tractogram.streamlines\n",
    "hdr = tractogram.header\n",
    "print(streamlines_rasmm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Streamlines in Voxel Coordinates to Trackvis Voxmm Space\n",
    "We multiply each point by the images voxel size and translate by half the voxel size.  The origin (point [0, 0, 0]) in voxel coordinates refers to the center of the first voxel.  The origin after this conversion refers to the outer corner of the first voxel. Point (0, 0, 0) in voxel coordinates would be (0.5, 0.5, 0.5) in trackvis voxmm coordinates if the voxel sizes were (1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.tracking.streamline import transform_streamlines\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "\n",
    "# Construct vox_to_voxmm affine\n",
    "voxel_size = np.asarray(img.header.get_zooms()[0:3])\n",
    "vox_to_voxmm = np.eye(4)\n",
    "vox_to_voxmm[[0, 1, 2], [0, 1, 2]] = voxel_size\n",
    "vox_to_voxmm[:3, 3] = voxel_size / 2\n",
    "print(vox_to_voxmm)\n",
    "\n",
    "# Transform streamlines\n",
    "streamlines_voxmm_new = transform_streamlines(streamlines_voxel, vox_to_voxmm)\n",
    "\n",
    "# Assert that transformed streamlines equals the original streamlines (ones in grid space)\n",
    "assert_array_almost_equal(streamlines_voxmm_new[0], streamlines_voxmm[0], decimal=3)\n",
    "print(streamlines_voxmm_new[0])\n",
    "print(streamlines_voxmm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Streamlines in Voxel Coordinates to RASMM World Coordinates\n",
    "Here we simply transform each streamline point by the images affine (qfrom) which takes the image from grid coordinates to world coordinates.  Again the qform maps to either scanner coordinates or coordinates in relation to some template ex MNI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.tracking.streamline import transform_streamlines\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "\n",
    "# Get affine and transform streamlines\n",
    "vox_to_rasmm = img.affine\n",
    "streamlines_rasmm_new = transform_streamlines(streamlines_voxel, vox_to_rasmm)\n",
    "\n",
    "# Assert new streamlines are equal to the ones we loaded in rasmm\n",
    "assert_array_almost_equal(streamlines_rasmm_new[0], streamlines_rasmm[0], decimal=3)\n",
    "print(streamlines_rasmm_new[0])\n",
    "print(streamlines_rasmm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Tracking\n",
    "\n",
    "Lets get more complicated! Dipy's eudx algorithm is its most basic tractography algorithm.  Its been superseded by their local tracking algorithm.  You won't even find eudx in the documentation anymore.  Local tracking lets you define seeding regions, and lets you specifiy whether you want your direction getter to be probabilistic or deterministic. It also lets you define a tissue classifer (which we can use to further constrain the the tracks we generate). \n",
    "\n",
    "While we can use more complicated reconstruction techniques with eudx, we'll show how to do them with local tracking. Let's start off by reconstructing our diffusion data with the Constrained Sphereical Deconvolution (CSD) approach.  \n",
    "\n",
    "### Fit the CSD model to the data and get spherical harmonic coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.reconst.csdeconv import auto_response\n",
    "from dipy.reconst.csdeconv import ConstrainedSphericalDeconvModel\n",
    "\n",
    "response, ratio = auto_response(gtab, data, roi_radius=10, fa_thr=0.7)\n",
    "print(ratio)\n",
    "\n",
    "csd_model = ConstrainedSphericalDeconvModel(gtab, response)\n",
    "csd_fit = csd_model.fit(data)\n",
    "shm_coeff = csd_fit.shm_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize ODFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.data import get_sphere\n",
    "from dipy.viz import window, actor\n",
    "\n",
    "# Get Sphere and use to get odfs from CSDFit object\n",
    "sphere = get_sphere('symmetric724')\n",
    "odfs = csd_fit.odf(sphere)\n",
    "csd_odf_small = odfs[:, :, 50:51]\n",
    "\n",
    "# Visualization\n",
    "ren = window.Renderer()\n",
    "fodf_spheres = actor.odf_slicer(csd_odf_small, sphere=sphere, scale=0.9, norm=False, colormap='plasma')\n",
    "ren.add(fodf_spheres)\n",
    "window.show(ren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Tracking Using A Threshohld Tissue Classifier\n",
    "\n",
    "For a first shot, lets use local tracking with the typcial threshold tissue classifier.  This means we'll use a scalar map (FA) to tell the algorithm when to stop tracking. To make things a little more interesting, lets add the following specs\n",
    "* Probabilistic Direction Getter\n",
    "* Seed From ONLY white matter\n",
    "* Seed 1 time per voxel, and generate seeds randomly within each voxel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.tracking.local import ThresholdTissueClassifier\n",
    "from dipy.tracking.local import LocalTracking\n",
    "from dipy.direction import ProbabilisticDirectionGetter\n",
    "from dipy.tracking.utils import random_seeds_from_mask\n",
    "from dipy.io.streamline import save_trk\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "import numpy as np\n",
    "\n",
    "# Define the probabilistic direction getter\n",
    "pg = ProbabilisticDirectionGetter.from_shcoeff(shm_coeff, max_angle=40., sphere=sphere)\n",
    "\n",
    "# We have a white matter pve map from FSL's FAST Algorithm\n",
    "f_white_matter_pve = './data/002_S_0413_1_T1_masked_N3_2Colins_6p_pve_2.nii.gz'\n",
    "wm_pve_img = nib.load(f_white_matter_pve)\n",
    "wm_pve_data = wm_pve_img.get_data()\n",
    "\n",
    "# We only want values over 0.9, this becomes our seeding mask\n",
    "seed_mask = np.ones(wm_pve_data.shape)\n",
    "seed_mask[wm_pve_data < 0.9] = 0\n",
    "\n",
    "# Lets generate 1 seed per voxel randomly distributed in the voxel from the wm mask\n",
    "seeds = random_seeds_from_mask(seed_mask, seeds_count=1, affine=np.eye(4))\n",
    "\n",
    "# Define the tissue classifier\n",
    "fa = ten.fa\n",
    "classifier = ThresholdTissueClassifier(fa, .20)\n",
    "\n",
    "# Compute tracks\n",
    "streamlines = LocalTracking(pg, classifier, seeds, np.eye(4), step_size=0.5)\n",
    "streamlines = Streamlines(streamlines)\n",
    "print(streamlines[0])\n",
    "\n",
    "# Save tracks\n",
    "save_trk(\"Probabilistic_WMseeding_FAthresh.trk\", streamlines, img.affine, vox_size=img.header.get_zooms()[0:3], shape=data.shape[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Probabilistic Tracks with FA threshold\n",
    "Again we can use dipy's viz api to look at what we just generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "from dipy.viz.colormap import line_colors\n",
    "\n",
    "color = line_colors(streamlines)\n",
    "streamlines_actor = actor.line(streamlines, line_colors(streamlines))\n",
    "r = window.Renderer()\n",
    "r.add(streamlines_actor)\n",
    "window.show(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Tracking Using A Binary Tissue Classifier\n",
    "\n",
    "Next lets use a binary tissue classifier instead of a scalar map. \n",
    "\n",
    "* Seed from only white matter like before\n",
    "* But lets make a deterministic direction getter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.direction import DeterministicMaximumDirectionGetter\n",
    "from dipy.tracking.local import BinaryTissueClassifier\n",
    "\n",
    "# Make white mattter mask\n",
    "f_white_matter_pve = './data/002_S_0413_1_T1_masked_N3_2Colins_6p_pve_2.nii.gz'\n",
    "wm_pve_img = nib.load(f_white_matter_pve)\n",
    "wm_pve_data = wm_pve_img.get_data()\n",
    "white_matter_mask = np.ones(wm_pve_data.shape)\n",
    "white_matter_mask[wm_pve_data < 0.3] = 0\n",
    "\n",
    "# Define direction getter with spherical harmonics\n",
    "dg = DeterministicMaximumDirectionGetter.from_shcoeff(shm_coeff, max_angle=40., sphere=sphere)\n",
    "\n",
    "# Define the tissue classifier\n",
    "binary_classifier = BinaryTissueClassifier(white_matter_mask)\n",
    "\n",
    "# Compute streamlines\n",
    "streamlines = LocalTracking(dg, binary_classifier, seeds, np.eye(4), step_size=0.5)\n",
    "streamlines = Streamlines(streamlines)\n",
    "save_trk(\"Determinsitc_WMseeding_Binarythresh.trk\", streamlines, img.affine, vox_size=img.header.get_zooms()[0:3],  shape=data.shape[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Again...\n",
    "Lets use dipy's api ... again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "from dipy.viz.colormap import line_colors\n",
    "\n",
    "color = line_colors(streamlines)\n",
    "streamlines_actor = actor.line(streamlines, line_colors(streamlines))\n",
    "r = window.Renderer()\n",
    "r.add(streamlines_actor)\n",
    "window.show(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Tracking Using ACT (Anatomically Constrained Tractography)\n",
    "Now lets get really fancy.  We're going to compute Anatomically Constrained Tractography by using all three PVE maps output by FSL's Fast algorithm.  Lets also get fancy with our seeding mask.  We'll use the following specs\n",
    "* Deterministic Direction Getter\n",
    "* Seed from the interface between the gray and white matter\n",
    "* Seed 1 times per voxel, again randomly distributed within each voxel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load our pve data - we've already loaded the white matter\n",
    "f_wm_pve = './data/002_S_0413_1_T1_masked_N3_2Colins_6p_pve_2.nii.gz'\n",
    "f_gm_pve = './data/002_S_0413_1_T1_masked_N3_2Colins_6p_pve_1.nii.gz'\n",
    "f_csf_pve = './data/002_S_0413_1_T1_masked_N3_2Colins_6p_pve_0.nii.gz'\n",
    "wm_pve_img = nib.load(f_wm_pve)\n",
    "gm_pve_img = nib.load(f_gm_pve)\n",
    "csf_pve_img = nib.load(f_csf_pve)\n",
    "wm_pve_data = wm_pve_img.get_data()\n",
    "gm_pve_data = gm_pve_img.get_data()\n",
    "csf_pve_data = csf_pve_img.get_data()\n",
    "\n",
    "# Show them side by side\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"White Matter\")\n",
    "plt.imshow(wm_pve_data[:, :, wm_pve_data.shape[2] // 2].T, cmap='gray', origin='lower',\n",
    "           interpolation='nearest')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Gray Matter')\n",
    "plt.imshow(gm_pve_data[:, :, gm_pve_data.shape[2] // 2].T, cmap='gray', origin='lower',\n",
    "           interpolation='nearest')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('CSF')\n",
    "plt.imshow(csf_pve_data[:, :, csf_pve_data.shape[2] // 2].T, cmap='gray', origin='lower',\n",
    "           interpolation='nearest')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Seeding Mask in White Gray Matter Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create our seeding mask\n",
    "seedmask = np.zeros(wm_pve_data.shape)\n",
    "seedmask[(wm_pve_data > 0.05) & (gm_pve_data > 0.05)] = 1\n",
    "\n",
    "# Show Seeding mask\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Interface Between White and Gray Matter\")\n",
    "plt.imshow(seedmask[:, :, seedmask.shape[2] // 2].T, cmap='gray', origin='lower',\n",
    "           interpolation='nearest')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Direction Getter, Tissue Classifier, and Compute Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.tracking.local import ActTissueClassifier\n",
    "from dipy.direction import DeterministicMaximumDirectionGetter\n",
    "from dipy.tracking.local import LocalTracking\n",
    "from dipy.tracking.utils import random_seeds_from_mask\n",
    "from dipy.data import default_sphere\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "from dipy.io.streamline import save_trk\n",
    "\n",
    "# Instantiate Direction Getter\n",
    "dg = DeterministicMaximumDirectionGetter.from_shcoeff(shm_coeff, max_angle=40., sphere=default_sphere)\n",
    "\n",
    "# Get Seeds\n",
    "seeds = random_seeds_from_mask(seedmask, seeds_count=1, affine=np.eye(4))\n",
    "\n",
    "# Generate Include and Exclude Maps for ACT Classifier\n",
    "include_map = gm_pve_data\n",
    "exclude_map = csf_pve_data\n",
    "\n",
    "# Add background so that tracks can exit the brain through cst\n",
    "background = np.ones(wm_pve_data.shape)\n",
    "background[(gm_pve_data +\n",
    "            wm_pve_data +\n",
    "            csf_pve_data) > 0] = 0\n",
    "include_map[background > 0] = 1\n",
    "\n",
    "# Instantiate classifier\n",
    "act_classifier = ActTissueClassifier(include_map, exclude_map)\n",
    "\n",
    "# Compute Tracks\n",
    "streamlines = LocalTracking(dg, act_classifier, seeds, np.eye(4), step_size=0.5, return_all=False)\n",
    "streamlines = Streamlines(streamlines)\n",
    "\n",
    "# Save Tracks\n",
    "save_trk(\"Deterministic_ACT_Classifier.trk\", streamlines, img.affine, vox_size=img.header.get_zooms()[0:3],  shape=data.shape[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### And finally... we visualize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "from dipy.viz.colormap import line_colors\n",
    "\n",
    "color = line_colors(streamlines)\n",
    "streamlines_actor = actor.line(streamlines, line_colors(streamlines))\n",
    "r = window.Renderer()\n",
    "r.add(streamlines_actor)\n",
    "window.show(r)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
