{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Basics: Spreadsheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Library: csv](#csv)\n",
    "* [Library: pandas](#pandas)\n",
    "    * [Reading csv/tsv](#pdcsv)\n",
    "    * [Reading excel](#excel)\n",
    "    * [DataFrames](#df)\n",
    "    * [Useful Methods](#methods)\n",
    "    * [Writing Files](#writing)\n",
    "    \n",
    "### How to Succeed in Business Without Really Crying   \n",
    "\n",
    "All of us deal with spreadsheets. Excel and the like are very nice GUIs/programs that help us deal with them, but sometimes it's nice to not have to deal things manually and/or have a record of what we've done and be able to reproduce it (especially so we don't have to cry if we haven't saved recently and the program crashes in the middle of our work).\n",
    "\n",
    "Technically we can use Python's reader and writer objects to handle some of them (e.g., comma- or tab-separated value files), but there are some libraries that can help make the process a little easier on us.\n",
    "\n",
    "Let's go through a few ways to do things as painfree as possible. We're going to focus on the [pandas library](#pandas), but the [csv library](#csv) will be covered just for completeness' sake.\n",
    "\n",
    "## Library: csv <a class=\"anchor\" id=\"csv\"></a>\n",
    "\n",
    "<a href=\"https://docs.python.org/3/library/csv.html\">csv</a> is a part of the Python Standard Library (i.e., it comes with any and all installations of python).  As the name indicates, this library is specifically designed to help us with csv (comma-separated value) files, although we can also change the delimiter to deal with tab-separated value files. There's a lot going on on the reference website, so let's just learn the essentials through some examples.\n",
    "\n",
    "### Reading Files\n",
    "\n",
    "There are two ways to read in csv files:\n",
    "\n",
    "    csv.reader()\n",
    "    csv.DictReader()\n",
    "    \n",
    "<em>csv.reader( )</em> isn't that much more useful than reading in the csv file like a regular old text file. In all likelihood, the only noticeable difference is that it reads each row as a list of strings rather than a single string.\n",
    "\n",
    "    row1 = [\"value1\", \"value2\", ...]\n",
    "\n",
    "<em>csv.DictReader( )</em> is nicer in that it parses the lines into a dictionary, taking the column headers into consideration. \n",
    "    \n",
    "    row1 = {\"column1\": \"value1\", \"column2\": \"value2\", ...}\n",
    "\n",
    "If you'd like to see more on how to use them, follow the link above to the csv library website.\n",
    "\n",
    "### Writing Files\n",
    "\n",
    "There are also two ways to write csv files:\n",
    "\n",
    "    csv.writer()\n",
    "    csv.DictWriter()\n",
    "    \n",
    "They operate in much the same way that their reading counterparts do. <em>csv.DictWriter( )</em> is probably the most useful tool here because instead of paying attention to making sure all cells are in the right order and empty ones are accounted for, we can write whatever we want in any order and the dictionary will ensure that everything's placed correctly. \n",
    "\n",
    "    csvwriter.writerow({\"column3\": \"value3\", \"column1\": \"value1\"}) \n",
    "    # there's no assigment for column2, the dictionary items are out-of-order, and that's all fine\n",
    "\n",
    "Once again, head on over to the website if you'd like more information.\n",
    "            \n",
    "## Library: pandas <a class=\"anchor\" id=\"pandas\"></a>\n",
    "\n",
    "<a href=\"http://pandas.pydata.org/pandas-docs/stable/\">pandas</a> is not part of the Python Standard Library, but it does come with the <a href=\"https://conda.io/docs/user-guide/install/download.html\">Anaconda</a> distribution. As mentioned before, this is where the focus of this tutorial lies because it's capable of so much more.\n",
    "\n",
    "We'll go over reading/writing csv, tsv, and excel files, but there are <a href=\"https://pandas.pydata.org/pandas-docs/stable/io.html\">many other formats</a> that pandas can handle. After reading in a spreadsheet file, pandas returns a dataframe. If you're not familiar with dataframes, we'll take about them more in a bit. \n",
    "\n",
    "Pandas is typically imported with the nickname <font color=\"blue\"><em>pd</em></font>, and that's what we'll generally see when we look at package documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # We'll also load this to give us more versatility in the numbers game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files\n",
    "\n",
    "#### csv/tsv<a class=\"anchor\" id=\"pdcsv\"></a>\n",
    "\n",
    "Reading csv files is really easy.\n",
    "\n",
    "    pd.read_csv()\n",
    "    \n",
    "The command does what it sounds like. If we'd really like to get fancy and see all the available options, we can run <font color=\"blue\"><em>pd.read_csv?</em></font> in ipython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot going on there, so we'll just highlight a few that may be useful.\n",
    "\n",
    "    delimiter: the program will try to determine what it is for you, but you can also explicitly tell it what to use\n",
    "\n",
    "    header: default is to use the first row, but if there are notes at the top and we want row n to be our header, \n",
    "            then we can use this parameter, e.g., header=2 (remember python uses zero-based indexing, so this is \n",
    "            row 3)\n",
    "            \n",
    "    skiprows: default is to not skip any, but if there's extraneous text for a line or two, you can also tell  \n",
    "              pandas to ignore them\n",
    "    \n",
    "    index_col: default is to create a new index of just plain ol' sequential integers for each row, but if we'd \n",
    "            like to index by a specific column (e.g., subject IDs), then use this parameter\n",
    "            \n",
    "    names: default is None because typically we have column headers, but if we don't, this is the parameter to\n",
    "            use; give it a list of column names, and it'll use those instead of any given row\n",
    "\n",
    "In all likelihood, we don't need to get too fancy, so let's move on and start with some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(\"/ifs/loni/faculty/thompson/four_d/igc_basics/python/nonsensical_example_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for how the data we've just read in will look like. I'm using the <font color=\"blue\"><em>.head( )</em></font> method to just display the top five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading tsv files or other table-formatted text files is equally as easy.\n",
    "\n",
    "    pd.read_table()\n",
    "    \n",
    "All the aforementioned parameters are available with this command as well.\n",
    "\n",
    "#### excel <a class=\"anchor\" id=\"excel\"></a>\n",
    "\n",
    "Excel spreadsheets are binary files, not text files. Luckily for us, that's not a problem for pandas.\n",
    "\n",
    "There are two ways for us to read in Excel spreadsheets.\n",
    "\n",
    "If we don't know how many sheets an Excel file has or what they're called, then we should use <font color=\"blue\"><em>pd.ExcelFile( )</em></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excel_file = pd.ExcelFile(\"/ifs/loni/faculty/thompson/four_d/igc_basics/python/nonsensical_example_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the <font color=\"blue\"><em>.sheet_names</em></font> attribute to see what sheets are available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_file.sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can grab the data of any specific sheet using the <font color=\"blue\">.parse( )<em></em></font> method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_file.parse(\"site1\").head()     # once again using .head() to only display a subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we already know the sheetnames and which one we want, we can straightshot the whole process and use <font color=\"blue\"><em>pd.read_excel(ExcelFile, sheetname=\"sheetName\")</em></font>. If a specific sheet is not specified (<font color=\"blue\"><em>pd.read_excel(ExcelFile)</em></font>), this command will read in the first sheet by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data = pd.read_excel(\"/ifs/loni/faculty/thompson/four_d/igc_basics/python/nonsensical_example_data.xlsx\")\n",
    "\n",
    "excel_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_excel(\"/ifs/loni/faculty/thompson/four_d/igc_basics/python/nonsensical_example_data.xlsx\", sheetname=\"site2\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames<a class=\"anchor\" name=\"df\"></a>\n",
    "\n",
    "\n",
    "Okay, so what actually are these DataFrames that we've been talking about? We saw examples of them above, but let's talk about what the components are and how to call them. \n",
    "\n",
    "Columns are straightforward. They are what they sound like. We can find out what the column names are with the attribute:\n",
    "\n",
    "    df.columns\n",
    "\n",
    "To grab a specific column, we can use\n",
    "\n",
    "    df['columnName']\n",
    "    \n",
    "The nice thing about this method is that we can use it regardless of what's in the column name (spaces, punctuation, etc.).\n",
    "\n",
    "    df['column name']\n",
    "    \n",
    "If a column name doesn't have punctuation or spaces, we can also use\n",
    "\n",
    "    df.columnName\n",
    "    \n",
    "All of these will return a pandas.Series object. See below for what that looks like and how to turn it into a list.\n",
    "    \n",
    "The index is the row identifier. They can be viewed using\n",
    "\n",
    "    df.index\n",
    "    \n",
    "We can use the index and column name to access a given cell or selection of cells.\n",
    "\n",
    "    df.loc[index,columnName]\n",
    "    \n",
    "Each cell is allowed to be its own data type, which pandas infers when it loads in the dataset. We can also set the data types explicitly, but we probably won't have to.\n",
    "\n",
    "There can also be multi-level indexing with both the rows and columns using tuples. That might sound difficult or scary, so let's table that for now. Just know that it can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.SubjectID  # pandas.Series object that contains both the index and the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.SubjectID.values    # an array with the values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.SubjectID.values.tolist()    # our values as a simple list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Methods<a class=\"anchor\" name=\"methods\"></a>\n",
    "\n",
    "Let's just read in all the data from our excel file into one dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "\n",
    "for sn in excel_file.sheet_names:\n",
    "    all_data[sn] = excel_file.parse(sn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sn, sn_data in all_data.items():\n",
    "    print(sn, sn_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that site 3 is an outlier in how it capitalized the column \"handedness\". We can change that to make everything consistent.\n",
    "\n",
    "We'll use the <font color=\"blue\"><em>.rename( )</em></font> method, which let's us give a dictionary of replacement names to the argument columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data['site3'].rename(columns={\"handedness\": \"Handedness\"}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, cool, that worked.\n",
    "\n",
    "BUT one thing that's important to keep in mind with pandas is that they really like dataframes to stay as there are. In other words, if we do something like replace a column name, it will expect you to do a variable (re-)assignment.\n",
    "\n",
    "Let's demonstrate that by looking at the columns names again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sn, sn_data in all_data.items():\n",
    "    print(sn, sn_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "site3 still has \"handedness\" as all lowercase. To make the changes stick, we'll need to either create a new variable or reassign our edited dataframe to the old variable name.\n",
    "\n",
    "For a lot of pandas methods, however, there is a handy flag called <font color=\"blue\"><em>inplace</em></font>. Set that to True to keep the old variable name but with the newly changed dataframe (it's False by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['site3'].rename(columns={\"handedness\": \"Handedness\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sn, sn_data in all_data.items():\n",
    "    print(sn, sn_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding New Columns\n",
    "\n",
    "Let's say that we want to add a new column, Age_squared. We can do that the same way we would with a dictionary. \n",
    "\n",
    "    dataFrame[\"new_column\"] = list or array of values to fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excel_data[\"Age_squared\"] = excel_data[\"Age\"] **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.loc[:, [\"Age\", \"Age_squared\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Columns/Rows\n",
    "\n",
    "If we'd like to drop columns, we can use the <font color=\"blue\"><em>.drop( )</em></font> method. One important flag to keep in mind here is the axis. 0 refers to rows (we're deleting a row with a given index), and 1 refers to columns (we're deleting a column with a given header name). The default is axis=0, so if we try to drop a column and forget to change that axis value, we'll run into an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making a fake column for us to drop\n",
    "\n",
    "excel_data['dummy'] = ['blah' for i in excel_data.index]\n",
    "\n",
    "excel_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "excel_data.drop('dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.drop('dummy', axis=1, inplace=True)#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Unique Values\n",
    "\n",
    "We can find unique values of a column by using the <font color=\"blue\"><em>.unique( )</em></font> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.Handedness.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing Existing Cells\n",
    "\n",
    "We can edit existing an individual or subset of cells by using the <font color=\"blue\"><em>.loc[ ]</em></font> method.\n",
    "\n",
    "If there's a mass replace we want to do though, we can also use the <font color=\"blue\"><em>.replace(someValue, anotherValue)</em></font> method.\n",
    "\n",
    "Let's take a look at site2's Handedness column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"site2\"].Handedness.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw above that site1 and site3 use R/L/A. Let's use the .replace( ) method to force consistency.\n",
    "\n",
    "First, let's see what the values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"site2\"].Handedness.unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could replace each value individually as written in the usage above:\n",
    "\n",
    "    all_data[\"site2\"].Handedness.replace(\"right\", \"R\")\n",
    "    all_data[\"site2\"].Handedness.replace(\"left\", \"L\")\n",
    "    all_data[\"site2\"].Handedness.replace(\"ambi\", \"A\")\n",
    "\n",
    "Or we could give replace a dictionary and do it all in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data[\"site2\"].Handedness.replace({\"right\": \"R\", \"left\": \"L\", \"ambi\": \"A\"}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better. But remember: we need to use the inplace flag to make it stick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"site2\"].Handedness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"site2\"].Handedness.replace({\"right\": \"R\", \"left\": \"L\", \"ambi\": \"A\"}, inplace=True)\n",
    "\n",
    "all_data[\"site2\"].Handedness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "excel_data[excel_data.ICV > 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data[(excel_data.ICV > 1500) & (excel_data.Handedness == \"R\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby\n",
    "\n",
    "All right, so let's so we want to look at how many people are of any given handedness. We can use the <font color=\"blue\"><em>.groupby( )</em></font> method and then the <font color=\"blue\"><em>.count( )</em></font> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "excel_data.groupby(\"Handedness\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 2 ambidextrous people, 3 left handers, and 15 right handers.\n",
    "\n",
    "Now let's take a look at site3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data[\"site3\"].groupby(\"Handedness\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... something's not right there. Time to check in with site3.\n",
    "\n",
    "We can also use the <font color=\"blue\"><em>.mean( )</em></font> method to see the mean values of our columns for each handedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.groupby(\"Handedness\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're only interested in on column, we can just call that one too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.groupby(\"Handedness\").mean()[\"ICV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excel_data.groupby?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of other methods that <font color=\"blue\"><em>.groupby( )</em></font> has. Check out <a href=\"https://pandas.pydata.org/pandas-docs/stable/groupby.html#groupby-object-attributes\">this webpage</a> for others.\n",
    "\n",
    "We can even apply our own using <font color=\"blue\"><em>.groupby( ).apply(ourOwnFunction)</em></font>. ourOwnFunction is often a lambda function. We'll do an example later.\n",
    "\n",
    "#### Merging Files: Add as Columns\n",
    "\n",
    "Time to leave VLOOKUP( ) behind and use <font color=\"blue\"><em>pd.merge( )</em></font> to combine spreadsheets/data with common identifiers.\n",
    "    \n",
    "Let's take a look at sheets site1 and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"site1\"].SubjectID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data[\"processed\"].SubjectID.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed seems to have some extra stuff attached, so let's get rid of that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data[\"processed\"][\"SubjectID\"] = all_data[\"processed\"][\"SubjectID\"].str.split(\"_\").str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"processed\"].SubjectID.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Let's merge now.\n",
    "\n",
    "<font color=\"blue\"><em>pd.merge( )</em></font> has a lot of different ways of merging. Take a look <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\">over here</a> for a complete breakdown. \n",
    "\n",
    "We'll go over a few examples to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.merge(all_data[\"site1\"], all_data[\"processed\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy, but what if they didn't have the index and SubjectID column in common. What if one of them was subID? Then we could use the <font color=\"blue\"><em>left_on</em></font> and <font color=\"blue\"><em>right_on</em></font> parameters. Left refers to the first mentioned dataframe, and right to the second.\n",
    "\n",
    "    pd.merge(dataFrame1, dataFrame2, left_on=\"SubjectID\", right_on=\"subID\")\n",
    "    \n",
    "And if our subject IDs were loaded in as our index, then we could use <font color=\"blue\"><em>left_index=True</em></font> and <font color=\"blue\"><em>right_index=True</em></font>.\n",
    "\n",
    "We can mix and match them however suits us best (e.g., index for one, column for another).\n",
    "\n",
    "By default, <font color=\"blue\"><em>pd.merge</em></font> uses the intersection of the keys (<font color=\"blue\"><em>how=\"inner\"</em></font>); however, you can change that to only using those of one dataframe (<font color=\"blue\"><em>how=\"left\"</em></font> or <font color=\"blue\"><em>how=\"right\"</em></font>) or the union of both (<font color=\"blue\"><em>how=\"outer\"</em></font>).\n",
    "\n",
    "As with before we'll need to use the <font color=\"blue\"><em>inplace=True</em></font> flag to make the changes stick or assign our changed dataframe to a new variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Files: Add as Rows\n",
    "\n",
    "All right now, let's combine the data from all of our sites into one dataframe. We can do that using <font color=\"blue\"><em>pd.concat([list of dataframes to combine])</em></font>. By default, <font color=\"blue\"><em>pd.concat( )</em></font> does a union/outer combination, so that's great news for us.\n",
    "\n",
    "First though, let's add a column to each with the site name, so we don't lose track of which subject came from where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"site1\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for site in [\"site1\", \"site2\", \"site3\"]:\n",
    "    all_data[site][\"Site\"] = [site for i in all_data[site].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"site1\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"site2\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"site3\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat([all_data[\"site1\"], all_data[\"site2\"], all_data[\"site3\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how our index is no longer unique to each subject. Since our index isn't meaningful anyway, we can use the parameter <font color=\"blue\"><em>ignore_index=True</em></font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.concat([all_data[\"site1\"], all_data[\"site2\"], all_data[\"site3\"]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better.\n",
    "\n",
    "#### Dealing with Empty Cells\n",
    "\n",
    "pandas will assign each empty cell as NA. \n",
    "\n",
    "If we'd like to drop rows with empty cells, we can use <font color=\"blue\"><em>.dropna( )</em></font>. To drop columns with empty cells, we can change the axis: <font color=\"blue\"><em>.dropna(axis=1)</em></font>. By default, this will be applied to the whole dataframe. We can also apply this to a subset of columns <font color=\"blue\"><em>.dropna(subset=[list of columns to apply this to])</em></font>. Also by default, this will drop occur using the method \"any\", i.e., if any cell in this row or column has an NA, it's gone. We can finetune that by using <font color=\"blue\"><em>.dropna(how=\"all\")</em></font> to get rid of only rows/columns that are empty or <font color=\"blue\"><em>.dropna(thresh=someInteger)</em></font> to get rid of rows/columns that have >=someInteger NAs in them.\n",
    "\n",
    "If we'd like to fill our empty cells, we can use <font color=\"blue\"><em>.fillna(fillValue)</em></font>. <font color=\"blue\"><em>.fillna( )</em></font> doesn't have a subset option (or any of the other ones listed above other than axis), but we can do that anyway by using <font color=\"blue\"><em>dataFrame[\"columnName\"].fillna(fillValue)</em></font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeDF = pd.concat([all_data[\"site1\"], all_data[\"site2\"], all_data[\"site3\"]], ignore_index=True)\n",
    "\n",
    "completeDF['dummy'] = np.nan\n",
    "\n",
    "completeDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeDF.loc[4,\"dummy\"] = \"nonsense\"\n",
    "\n",
    "completeDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeDF.fillna(\"NA\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeDF.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeDF[\"dummy\"].fillna(\"NA\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating Through Rows\n",
    "\n",
    "To iterate through the rows, we'll use the <font color=\"blue\"><em>dataFrame.iterrows( )</em></font> method.\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # i is the index\n",
    "        # row is a dictionary with {\"columnName\": df.loc[i, \"columnName\"], ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completeDF[\"ICV2\"] = completeDF[\"ICV\"] *3/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.melt(completeDF, id_vars=[\"Site\", \"SubjectID\"], value_vars=[\"ICV\", \"ICV2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing DataFrames to Files<a class=\"anchor\" name=\"writing\"></a>\n",
    "\n",
    "If we'd like to write a single dataframe to a single file, then we can use either\n",
    "\n",
    "    dataFrame.to_csv(\"fileName.csv\")\n",
    "    dataFrame.to_excel(\"fileName.xlsx\")\n",
    "\n",
    "If we'd like to write multiple sheets of a single Excel file, then an extra step is required.\n",
    "\n",
    "    with pd.ExcelWriter('fileName.xlsx') as writer:\n",
    "        dataFrame1.to_excel(writer, sheet_name='SheetName1')\n",
    "        dataFrame2.to_excel(writer, sheet_name='SheetName2')\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "\n",
    "By default, the writer will include the index. If we're using one of the columns as an index, then we're cool. If however, we never specified an index, so it's a column of integers we don't care about, we can also tell the writer not to include it.\n",
    "\n",
    "    dataFrame.to_csv(\"fileName.csv\", index=False)\n",
    "    dataFrame.to_excel(\"fileName.xlsx\", index=False)\n",
    "    dataFrame1.to_excel(writer, sheet_name='SheetName1', index=False)\n",
    "    \n",
    "If we do want to write the index, however, but it doesn't have a name, then we can tell it to have a specific name.\n",
    "\n",
    "    dataFrame.to_csv(\"fileName.csv\", index_label='labelName')\n",
    "    dataFrame.to_excel(\"fileName.xlsx\", index_label='labelName')\n",
    "    dataFrame1.to_excel(writer, sheet_name='SheetName1', index_label='labelName')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
